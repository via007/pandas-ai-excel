import os

import gradio as gr
import pandas as pd

from main import (
    DEFAULT_API_BASE,
    DEFAULT_API_KEY,
    DEFAULT_MODEL,
    DEFAULT_PROVIDER,
    build_join_dataset,
    build_join_preview,
    build_profile_report,
    configure_llm,
    export_dataframe,
    get_dataset_columns,
    load_datasets,
    process_question,
    suggest_join_columns,
)

PLACEHOLDER = """
é—®é¢˜è¾“å…¥è¦è´´åˆExcelè¡¨è¿°ï¼Œå’ŒExcelæ— å…³çš„è¯é¢˜ä¼šå‡ºç°å¼‚å¸¸ï¼
æ”¯æŒå¤šæ–‡ä»¶ã€å¤šè¡¨ï¼šå¯å…ˆä¸Šä¼ å¤šä¸ªCSV/XLSXï¼Œå†é€‰æ‹©éœ€è¦åˆ†æçš„æ•°æ®é›†ã€‚
"""

OUTPUT_HINTS = {
    "è‡ªåŠ¨": "",
    "è¡¨æ ¼": "è¯·ä»¥è¡¨æ ¼å½¢å¼è¾“å‡ºç»“æœã€‚",
    "å›¾è¡¨": "è¯·ç”Ÿæˆå›¾è¡¨è¾“å‡ºç»“æœã€‚",
    "æ–‡æœ¬": "è¯·ç”¨ç®€æ´æ–‡æœ¬æ€»ç»“è¾“å‡ºç»“æœã€‚",
}

CHART_HINTS = {
    "æŠ˜çº¿å›¾": "è¯·ç”ŸæˆæŠ˜çº¿å›¾ã€‚",
    "æŸ±çŠ¶å›¾": "è¯·ç”ŸæˆæŸ±çŠ¶å›¾ã€‚",
    "é¥¼å›¾": "è¯·ç”Ÿæˆé¥¼å›¾ã€‚",
    "æ•£ç‚¹å›¾": "è¯·ç”Ÿæˆæ•£ç‚¹å›¾ã€‚",
    "ç›´æ–¹å›¾": "è¯·ç”Ÿæˆç›´æ–¹å›¾ã€‚",
}

PROVIDERS = {
    "OpenAI": {
        "api_base": "https://api.openai.com/v1",
        "models": ["gpt-4o-mini", "gpt-4o", "gpt-4.1-mini"],
    },
    "DeepSeek": {
        "api_base": "https://api.deepseek.com/v1",
        "models": ["deepseek-chat", "deepseek-reasoner"],
    },
    "é˜¿é‡Œäº‘ DashScope": {
        "api_base": "https://dashscope.aliyuncs.com/compatible-mode/v1",
        "models": ["qwen-max-0125", "qwen-plus", "qwen-turbo"],
    },
    "Gemini(OpenAIå…¼å®¹)": {
        "api_base": "https://generativelanguage.googleapis.com/v1beta/openai/",
        "models": ["gemini-1.5-flash", "gemini-1.5-pro"],
    },
    "è‡ªå®šä¹‰": {
        "api_base": "",
        "models": [],
    },
}

DEFAULT_PROVIDER = DEFAULT_PROVIDER if DEFAULT_PROVIDER in PROVIDERS else "è‡ªå®šä¹‰"
DEFAULT_API_KEY = DEFAULT_API_KEY

_provider_defaults = PROVIDERS.get(DEFAULT_PROVIDER, {})
if not DEFAULT_API_BASE:
    DEFAULT_API_BASE = _provider_defaults.get("api_base", "")
if not DEFAULT_MODEL:
    default_models = _provider_defaults.get("models", [])
    DEFAULT_MODEL = default_models[0] if default_models else ""


def _append_hint(question: str, hint: str) -> str:
    question = (question or "").strip()
    if not question:
        return hint
    return f"{question} {hint}"


def _dataset_updates(dataset_names):
    return gr.update(choices=dataset_names, value=dataset_names)


def _single_dataset_update(dataset_names):
    return gr.update(choices=dataset_names, value=dataset_names[0] if dataset_names else None)


def on_provider_change(provider: str):
    info = PROVIDERS.get(provider, {})
    api_base = info.get("api_base", "")
    models = info.get("models", [])
    model_value = models[0] if models else ""
    return gr.update(value=api_base), gr.update(choices=models, value=model_value)


def on_apply_config(provider: str, api_base: str, model: str, api_key: str):
    config = {
        "provider": provider,
        "api_base": api_base,
        "model": model,
        "api_key": api_key,
    }

    if provider == "è‡ªå®šä¹‰" and not api_base:
        return config, "è‡ªå®šä¹‰æ¨¡å¼éœ€è¦å¡«å†™ Base URL"

    ok, message = configure_llm(api_key, api_base, model)
    return config, message


def on_files_upload(files, load_all_sheets):
    if not files:
        empty_update = gr.update(choices=[], value=[])
        empty_single = gr.update(choices=[], value=None)
        empty_columns = gr.update(choices=[], value=None)
        return {}, None, empty_update, empty_single, None, "è¯·ä¸Šä¼ æ–‡ä»¶", empty_single, empty_single, empty_single, empty_columns, empty_columns

    file_paths = [f.name for f in files]
    datasets, summary_df, errors = load_datasets(file_paths, load_all_sheets)
    dataset_names = list(datasets.keys())

    preview_name = dataset_names[0] if dataset_names else None
    preview_df = datasets[preview_name].head(100) if preview_name else None
    left_name = dataset_names[0] if dataset_names else None
    right_name = dataset_names[1] if len(dataset_names) > 1 else left_name
    left_columns = get_dataset_columns(datasets, left_name) if left_name else []
    right_columns = get_dataset_columns(datasets, right_name) if right_name else []

    status_parts = [f"å·²åŠ è½½ {len(dataset_names)} ä¸ªæ•°æ®é›†"]
    if errors:
        status_parts.append(" | ".join(errors))

    return (
        datasets,
        summary_df,
        _dataset_updates(dataset_names),
        gr.update(choices=dataset_names, value=preview_name),
        preview_df,
        "ï¼›".join(status_parts),
        _single_dataset_update(dataset_names),
        _single_dataset_update(dataset_names),
        _single_dataset_update(dataset_names),
        gr.update(choices=left_columns, value=left_columns[0] if left_columns else None),
        gr.update(choices=right_columns, value=right_columns[0] if right_columns else None),
    )


def on_preview_change(preview_name, datasets):
    if not preview_name or preview_name not in datasets:
        return None
    return datasets[preview_name].head(100)


def on_profile_generate(dataset_name, datasets):
    overview, profile_df = build_profile_report(datasets, dataset_name)
    if overview is None:
        return "è¯·å…ˆé€‰æ‹©æ•°æ®é›†", None

    overview_text = (
        f"æ•°æ®é›†ï¼š{dataset_name}  |  è¡Œæ•°ï¼š{overview['rows']}  |  åˆ—æ•°ï¼š{overview['cols']}  |  "
        f"é‡å¤è¡Œï¼š{overview['duplicates']}  |  å†…å­˜ï¼š{overview['memory_mb']} MB"
    )
    return overview_text, profile_df


def on_profile_export(profile_df, export_formats):
    if profile_df is None or export_formats is None:
        return [], "è¯·å…ˆç”Ÿæˆæ•°æ®æ¦‚è§ˆ"
    if not export_formats:
        return [], "è¯·é€‰æ‹©å¯¼å‡ºæ ¼å¼"
    if isinstance(profile_df, list):
        profile_df = pd.DataFrame(profile_df)
    if isinstance(profile_df, dict):
        profile_df = pd.DataFrame(profile_df)
    if profile_df is None or profile_df.empty:
        return [], "æ²¡æœ‰å¯å¯¼å‡ºçš„æ¦‚è§ˆæ•°æ®"
    files = export_dataframe(profile_df, export_formats)
    return files, "å·²ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶"


def on_dataset_change(dataset_name, datasets):
    cols = get_dataset_columns(datasets, dataset_name)
    return gr.update(choices=cols, value=cols[0] if cols else None)


def on_auto_match(left_name, right_name, datasets):
    left_cols = get_dataset_columns(datasets, left_name)
    right_cols = get_dataset_columns(datasets, right_name)
    left_col, right_col = suggest_join_columns(left_cols, right_cols)
    if not left_col or not right_col:
        return None, None, "æœªæ‰¾åˆ°å¯è‡ªåŠ¨åŒ¹é…å­—æ®µ"
    return left_col, right_col, f"å·²åŒ¹é…å­—æ®µï¼š{left_col} = {right_col}"


def on_preview_join(datasets, left_name, right_name, left_col, right_col, join_type, preview_rows):
    preview_df, stats, join_config = build_join_preview(
        datasets,
        left_name,
        right_name,
        left_col,
        right_col,
        join_type,
        preview_rows,
    )
    return preview_df, stats, join_config


def on_save_join(datasets, join_config, new_name):
    new_datasets, final_name, error = build_join_dataset(datasets, join_config, new_name)
    if error:
        return (
            datasets,
            gr.update(choices=list(datasets.keys()), value=list(datasets.keys())),
            gr.update(choices=list(datasets.keys()), value=list(datasets.keys())[0] if datasets else None),
            gr.update(choices=list(datasets.keys()), value=list(datasets.keys())[0] if datasets else None),
            gr.update(choices=list(datasets.keys()), value=list(datasets.keys())[0] if datasets else None),
            gr.update(choices=list(datasets.keys()), value=list(datasets.keys())[0] if datasets else None),
            error,
        )

    dataset_names = list(new_datasets.keys())
    return (
        new_datasets,
        gr.update(choices=dataset_names, value=dataset_names),
        gr.update(choices=dataset_names, value=dataset_names[0] if dataset_names else None),
        gr.update(choices=dataset_names, value=dataset_names[0] if dataset_names else None),
        gr.update(choices=dataset_names, value=dataset_names[0] if dataset_names else None),
        gr.update(choices=dataset_names, value=dataset_names[0] if dataset_names else None),
        f"å·²ä¿å­˜ä¸ºæ–°æ•°æ®é›†ï¼š{final_name}",
    )


def on_submit(datasets, selected_names, question, output_mode, export_formats, llm_config):
    if not datasets:
        return "è¯·å…ˆä¸Šä¼ æ•°æ®æ–‡ä»¶", [], None
    if not selected_names:
        return "è¯·å…ˆé€‰æ‹©è¦åˆ†æçš„æ•°æ®é›†", [], None
    if not question or not question.strip():
        return "è¯·è¾“å…¥é—®é¢˜", [], None

    hint = OUTPUT_HINTS.get(output_mode, "")
    final_question = _append_hint(question, hint) if hint else question

    return process_question(datasets, selected_names, final_question, export_formats or [], llm_config)


with gr.Blocks(title="AI-Excelæ•°æ®å¤„ç†ä¸åˆ†æ", theme=gr.themes.Ocean()) as demo:
    gr.Markdown(
        """
        <div style="background-color: #e0f7fa; padding: 20px; border-radius: 8px; text-align: center;">
            <strong style="font-size: 18px;">ğŸ¤– DeepSeek + Excel æ•°æ®å¤„ç†</strong>
        </div>
        """
    )

    datasets_state = gr.State({})
    config_state = gr.State({
        "provider": DEFAULT_PROVIDER,
        "api_base": DEFAULT_API_BASE,
        "model": DEFAULT_MODEL,
        "api_key": DEFAULT_API_KEY,
    })
    join_state = gr.State(None)

    with gr.Tabs():
        with gr.Tab("æ•°æ®åˆ†æ"):
            with gr.Row():
                with gr.Column(scale=1):
                    file_upload = gr.Files(label="ä¸Šä¼ æ–‡ä»¶ï¼ˆæ”¯æŒcsv/xlsxï¼Œå¤šæ–‡ä»¶ï¼‰", file_types=[".csv", ".xlsx"])
                    load_all_sheets = gr.Checkbox(label="Excelè¯»å–æ‰€æœ‰Sheet", value=True)
                    upload_status = gr.Markdown()
                    dataset_summary = gr.Dataframe(label="æ•°æ®é›†åˆ—è¡¨", interactive=False)
                    preview_dataset = gr.Dropdown(label="é¢„è§ˆæ•°æ®é›†", choices=[])
                    data_display = gr.Dataframe(label="æ•°æ®é¢„è§ˆï¼ˆå‰100è¡Œï¼‰", interactive=False)

                with gr.Column(scale=1):
                    active_datasets = gr.Dropdown(label="é€‰æ‹©ç”¨äºåˆ†æçš„æ•°æ®é›†", multiselect=True, choices=[])
                    output_mode = gr.Dropdown(label="è¾“å‡ºåå¥½", choices=["è‡ªåŠ¨", "è¡¨æ ¼", "å›¾è¡¨", "æ–‡æœ¬"], value="è‡ªåŠ¨")
                    export_formats = gr.CheckboxGroup(
                        label="å¯¼å‡ºæ ¼å¼ï¼ˆè¡¨æ ¼: xlsx/csv/jsonï¼Œå›¾è¡¨: pngï¼‰",
                        choices=["xlsx", "csv", "json", "png"],
                        value=["xlsx", "csv"],
                    )
                    question_input = gr.Textbox(label="è¾“å…¥æ‚¨çš„é—®é¢˜", lines=8, placeholder=PLACEHOLDER)

                    with gr.Row():
                        btn_line = gr.Button("æŠ˜çº¿å›¾")
                        btn_bar = gr.Button("æŸ±çŠ¶å›¾")
                        btn_pie = gr.Button("é¥¼å›¾")
                        btn_scatter = gr.Button("æ•£ç‚¹å›¾")
                        btn_hist = gr.Button("ç›´æ–¹å›¾")

                    submit_button = gr.Button("æäº¤", variant="primary")
                    output_text = gr.Textbox(label="æ–‡æœ¬è¾“å‡º")
                    output_files = gr.Files(label="ä¸‹è½½æ–‡ä»¶")
                    output_image = gr.Image(label="å›¾ç‰‡è¾“å‡º")

        with gr.Tab("æ•°æ®æ¦‚è§ˆ"):
            gr.Markdown("é€‰æ‹©æ•°æ®é›†å¹¶ç”Ÿæˆæ¦‚è§ˆæŠ¥å‘Šã€‚")
            profile_dataset = gr.Dropdown(label="é€‰æ‹©æ•°æ®é›†", choices=[])
            generate_profile = gr.Button("ç”Ÿæˆæ¦‚è§ˆ", variant="primary")
            profile_overview = gr.Markdown()
            profile_table = gr.Dataframe(label="å­—æ®µæ¦‚è§ˆ", interactive=False, type="pandas")

            profile_export_formats = gr.CheckboxGroup(
                label="å¯¼å‡ºæ¦‚è§ˆæŠ¥å‘Š",
                choices=["xlsx", "csv", "json"],
                value=["xlsx"],
            )
            export_profile = gr.Button("å¯¼å‡ºæŠ¥å‘Š")
            profile_files = gr.Files(label="æŠ¥å‘Šæ–‡ä»¶")
            profile_status = gr.Markdown()

        with gr.Tab("å…³ç³»ä¸è§†å›¾"):
            gr.Markdown("é€‰æ‹©ä¸¤ä¸ªæ•°æ®é›†å¹¶é…ç½®è¿æ¥å­—æ®µï¼Œå¯é¢„è§ˆè¿æ¥æ•ˆæœå¹¶ä¿å­˜ä¸ºæ–°æ•°æ®é›†ã€‚")
            with gr.Row():
                left_dataset = gr.Dropdown(label="å·¦è¡¨", choices=[])
                right_dataset = gr.Dropdown(label="å³è¡¨", choices=[])

            with gr.Row():
                left_column = gr.Dropdown(label="å·¦è¡¨å­—æ®µ", choices=[])
                right_column = gr.Dropdown(label="å³è¡¨å­—æ®µ", choices=[])

            auto_match = gr.Button("è‡ªåŠ¨åŒ¹é…å­—æ®µ")
            auto_match_status = gr.Markdown()

            with gr.Row():
                join_type = gr.Dropdown(label="è¿æ¥æ–¹å¼", choices=["inner", "left", "right", "outer"], value="inner")
                preview_rows = gr.Slider(label="é¢„è§ˆè¡Œæ•°", minimum=10, maximum=200, value=50, step=10)

            preview_join = gr.Button("é¢„è§ˆè¿æ¥", variant="primary")
            relation_stats = gr.Markdown()
            join_preview = gr.Dataframe(label="è¿æ¥é¢„è§ˆ", interactive=False)

            new_dataset_name = gr.Textbox(label="ä¿å­˜ä¸ºæ–°æ•°æ®é›†åç§°")
            save_join = gr.Button("ä¿å­˜ä¸ºæ–°æ•°æ®é›†")
            save_status = gr.Markdown()

        with gr.Tab("é…ç½®"):
            gr.Markdown("å¡«å†™ API Key å¹¶é€‰æ‹©æ¨¡å‹åä¿å­˜é…ç½®ã€‚é»˜è®¤ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£ã€‚")
            provider = gr.Dropdown(
                label="LLM æä¾›æ–¹",
                choices=list(PROVIDERS.keys()),
                value=DEFAULT_PROVIDER,
            )
            api_base = gr.Textbox(label="Base URL", value=DEFAULT_API_BASE)
            model = gr.Dropdown(
                label="æ¨¡å‹",
                choices=PROVIDERS[DEFAULT_PROVIDER]["models"],
                value=DEFAULT_MODEL,
                allow_custom_value=True,
            )
            api_key = gr.Textbox(label="API Key", type="password", value=DEFAULT_API_KEY)
            save_config = gr.Button("ä¿å­˜é…ç½®", variant="primary")
            config_status = gr.Markdown()

    file_upload.change(
        on_files_upload,
        inputs=[file_upload, load_all_sheets],
        outputs=[
            datasets_state,
            dataset_summary,
            active_datasets,
            preview_dataset,
            data_display,
            upload_status,
            profile_dataset,
            left_dataset,
            right_dataset,
            left_column,
            right_column,
        ],
    )
    load_all_sheets.change(
        on_files_upload,
        inputs=[file_upload, load_all_sheets],
        outputs=[
            datasets_state,
            dataset_summary,
            active_datasets,
            preview_dataset,
            data_display,
            upload_status,
            profile_dataset,
            left_dataset,
            right_dataset,
            left_column,
            right_column,
        ],
    )

    preview_dataset.change(
        on_preview_change,
        inputs=[preview_dataset, datasets_state],
        outputs=data_display,
    )

    profile_dataset.change(
        on_profile_generate,
        inputs=[profile_dataset, datasets_state],
        outputs=[profile_overview, profile_table],
    )
    generate_profile.click(
        on_profile_generate,
        inputs=[profile_dataset, datasets_state],
        outputs=[profile_overview, profile_table],
    )
    export_profile.click(
        on_profile_export,
        inputs=[profile_table, profile_export_formats],
        outputs=[profile_files, profile_status],
    )

    left_dataset.change(
        on_dataset_change,
        inputs=[left_dataset, datasets_state],
        outputs=left_column,
    )
    right_dataset.change(
        on_dataset_change,
        inputs=[right_dataset, datasets_state],
        outputs=right_column,
    )
    auto_match.click(
        on_auto_match,
        inputs=[left_dataset, right_dataset, datasets_state],
        outputs=[left_column, right_column, auto_match_status],
    )
    preview_join.click(
        on_preview_join,
        inputs=[datasets_state, left_dataset, right_dataset, left_column, right_column, join_type, preview_rows],
        outputs=[join_preview, relation_stats, join_state],
    )
    save_join.click(
        on_save_join,
        inputs=[datasets_state, join_state, new_dataset_name],
        outputs=[
            datasets_state,
            active_datasets,
            preview_dataset,
            profile_dataset,
            left_dataset,
            right_dataset,
            save_status,
        ],
    )

    provider.change(
        on_provider_change,
        inputs=provider,
        outputs=[api_base, model],
    )
    save_config.click(
        on_apply_config,
        inputs=[provider, api_base, model, api_key],
        outputs=[config_state, config_status],
    )

    btn_line.click(lambda q: _append_hint(q, CHART_HINTS["æŠ˜çº¿å›¾"]), inputs=question_input, outputs=question_input)
    btn_bar.click(lambda q: _append_hint(q, CHART_HINTS["æŸ±çŠ¶å›¾"]), inputs=question_input, outputs=question_input)
    btn_pie.click(lambda q: _append_hint(q, CHART_HINTS["é¥¼å›¾"]), inputs=question_input, outputs=question_input)
    btn_scatter.click(lambda q: _append_hint(q, CHART_HINTS["æ•£ç‚¹å›¾"]), inputs=question_input, outputs=question_input)
    btn_hist.click(lambda q: _append_hint(q, CHART_HINTS["ç›´æ–¹å›¾"]), inputs=question_input, outputs=question_input)

    submit_button.click(
        on_submit,
        inputs=[datasets_state, active_datasets, question_input, output_mode, export_formats, config_state],
        outputs=[output_text, output_files, output_image],
    )


demo.launch()
